{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504405fd-6139-49ca-b7ad-97e7073758f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b384861e-0bc8-4b58-833b-9298abb4ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(data1, data2, data3, data4, common_key, target_column):\n",
    "    \"\"\"Charge, fusionne et nettoie les fichiers CSV.\"\"\"\n",
    "    caract = pd.read_csv(data1, sep=\";\", low_memory=False)\n",
    "    lieux = pd.read_csv(data2, sep=\";\", low_memory=False)\n",
    "    usagers = pd.read_csv(data3, sep=\";\", low_memory=False)\n",
    "    vehicules = pd.read_csv(data4, sep=\";\", low_memory=False)\n",
    "    \n",
    "    merged_df = caract.merge(lieux, on=common_key, how=\"inner\") \\\n",
    "                      .merge(usagers, on=common_key, how=\"inner\") \\\n",
    "                      .merge(vehicules, on=common_key, how=\"inner\")\n",
    "    \n",
    "    print(f\"Colonnes disponibles après fusion : {merged_df.columns.tolist()}\")\n",
    "\n",
    "    merged_df = merged_df.dropna(axis=1, how='all')\n",
    "    merged_df = merged_df.loc[:, merged_df.nunique() > 1]\n",
    "    \n",
    "    y = merged_df[target_column]\n",
    "    X = merged_df.drop(columns=[target_column, common_key])\n",
    "    return X, y\n",
    "\n",
    "def correlation_matrix(X, y, threshold=0.1):\n",
    "    \"\"\"Filtre les colonnes numériques basées sur leur corrélation avec la cible.\"\"\"\n",
    "    correlations = {}\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        corr = np.corrcoef(X[col], y)[0, 1]\n",
    "        correlations[col] = corr\n",
    "    selected = [col for col, corr in correlations.items() if abs(corr) > threshold]\n",
    "    print(f\"Colonnes numériques retenues (corrélation > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def categorical_analysis(X, y, threshold=0.2):\n",
    "    \"\"\"Filtre les colonnes catégorielles en se basant sur la variance des moyennes.\"\"\"\n",
    "    X_with_target = X.copy()\n",
    "    X_with_target['target'] = y\n",
    "\n",
    "    selected = []\n",
    "    for col in X.select_dtypes(include='object').columns:\n",
    "        means = X_with_target.groupby(col)['target'].mean()\n",
    "        if means.var() > threshold:\n",
    "            selected.append(col)\n",
    "    print(f\"Colonnes catégorielles retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def low_variance_filter(X, threshold=0.01):\n",
    "    \"\"\"Filtre les colonnes numériques avec une faible variance.\"\"\"\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    variances = X_numeric.var()\n",
    "    selected = variances[variances > threshold].index.tolist()\n",
    "    print(f\"Colonnes numériques retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "\n",
    "def auto_handle_nan(df, nan_threshold_delete=0.5, nan_threshold_impute=0.1):\n",
    "    \"\"\"\n",
    "    Traite automatiquement les valeurs NaN dans un dataset.\n",
    "    - Supprime les colonnes avec trop de NaN.\n",
    "    - Impute (remplace) les NaN avec des stratégies adaptées :\n",
    "      - Moyenne pour colonnes numériques.\n",
    "      - Mode ou \"Inconnu\" pour colonnes catégorielles.\n",
    "    \"\"\"\n",
    "    print(\"Analyse des NaN dans le dataset...\\n\")\n",
    "    df = df.replace(-1, 1)\n",
    "    nan_percent = df.isnull().mean()\n",
    "    print(\"Pourcentage de valeurs manquantes par colonne :\")\n",
    "    print(nan_percent)\n",
    "    \n",
    "    cols_to_delete = nan_percent[nan_percent > nan_threshold_delete].index\n",
    "    print(f\"\\nColonnes supprimées (trop de NaN > {nan_threshold_delete*100}%): {list(cols_to_delete)}\")\n",
    "    df = df.drop(columns=cols_to_delete)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if df[col].dtype == 'object':\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec 'Manquant' (catégorielle)\")\n",
    "                    df[col] = df[col].fillna(\"Manquant\")\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la valeur la plus fréquente (mode)\")\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la médiane (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la moyenne (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    print(\"\\nTraitement des NaN terminé.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4696f6e-5924-4dcb-9cd1-75e13bc64fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data1, data2, data3, data4, common_key, target_column):\n",
    "    print(\"Chargement et fusion des données...\")\n",
    "    X, y = load_and_process_data(data1, data2, data3, data4, common_key, target_column)\n",
    "\n",
    "    print(\"\\nSuppression des doublons...\")\n",
    "    print(\"Nombre de doublons avant suppression :\", X.duplicated().sum())\n",
    "    X = X.drop_duplicates()\n",
    "    print(\"Nombre de doublons après suppression :\", X.duplicated().sum())\n",
    "\n",
    "    print(\"\\nTraitement des valeurs NaN...\")\n",
    "    X = auto_handle_nan(X)\n",
    "\n",
    "    print(\"\\nSélection des colonnes numériques importantes...\")\n",
    "    numeric_cols_corr = correlation_matrix(X, y, threshold=0.1)\n",
    "    numeric_cols_var = low_variance_filter(X, threshold=0.1)\n",
    "\n",
    "    print(\"\\nSélection des colonnes catégorielles importantes...\")\n",
    "    categorical_cols = categorical_analysis(X, y, threshold=2)\n",
    "\n",
    "    selected_columns = list(set(numeric_cols_var + categorical_cols))\n",
    "    print(f\"\\nColonnes finales sélectionnées : {selected_columns}\")\n",
    "\n",
    "    X_filtered = X[selected_columns]\n",
    "    # final_data = pd.concat([X_filtered, y], axis=1)\n",
    "\n",
    "    print(\"\\nRésumé des colonnes importantes pour la prédiction :\")\n",
    "    print(f\"Nombre de colonnes finales : {len(X_filtered.columns)}\")\n",
    "    print(X_filtered.head())\n",
    "    return X_filtered, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e17c8-41d2-46ae-9fd9-e4595dd5be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du script...\n",
      "Chargement et fusion des données...\n",
      "Colonnes disponibles après fusion : ['Num_Acc', 'jour', 'mois', 'an', 'hrmn', 'lum', 'dep', 'com', 'agg', 'int', 'atm', 'col', 'adr', 'lat', 'long', 'catr', 'voie', 'v1', 'v2', 'circ', 'nbv', 'vosp', 'prof', 'pr', 'pr1', 'plan', 'lartpc', 'larrout', 'surf', 'infra', 'situ', 'vma', 'id_usager', 'id_vehicule_x', 'num_veh_x', 'place', 'catu', 'grav', 'sexe', 'an_nais', 'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'actp', 'etatp', 'id_vehicule_y', 'num_veh_y', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'motor', 'occutc']\n",
      "\n",
      "Suppression des doublons...\n",
      "Nombre de doublons avant suppression : 0\n",
      "Nombre de doublons après suppression : 0\n",
      "\n",
      "Traitement des valeurs NaN...\n",
      "Analyse des NaN dans le dataset...\n",
      "\n",
      "Pourcentage de valeurs manquantes par colonne :\n",
      "jour             0.000000\n",
      "mois             0.000000\n",
      "hrmn             0.000000\n",
      "lum              0.000000\n",
      "dep              0.000000\n",
      "com              0.000000\n",
      "agg              0.000000\n",
      "int              0.000000\n",
      "atm              0.000000\n",
      "col              0.000000\n",
      "adr              0.025318\n",
      "lat              0.000000\n",
      "long             0.000000\n",
      "catr             0.000000\n",
      "voie             0.163079\n",
      "v1               0.000000\n",
      "v2               0.917053\n",
      "circ             0.000000\n",
      "nbv              0.000000\n",
      "vosp             0.000000\n",
      "prof             0.000000\n",
      "pr               0.000000\n",
      "pr1              0.000000\n",
      "plan             0.000000\n",
      "lartpc           0.999483\n",
      "larrout          0.000000\n",
      "surf             0.000000\n",
      "infra            0.000000\n",
      "situ             0.000000\n",
      "vma              0.000000\n",
      "id_usager        0.000000\n",
      "id_vehicule_x    0.000000\n",
      "num_veh_x        0.000000\n",
      "place            0.000000\n",
      "catu             0.000000\n",
      "sexe             0.000000\n",
      "an_nais          0.020547\n",
      "trajet           0.000000\n",
      "secu1            0.000000\n",
      "secu2            0.000000\n",
      "secu3            0.000000\n",
      "locp             0.000000\n",
      "actp             0.000000\n",
      "etatp            0.000000\n",
      "id_vehicule_y    0.000000\n",
      "num_veh_y        0.000000\n",
      "senc             0.000000\n",
      "catv             0.000000\n",
      "obs              0.000000\n",
      "obsm             0.000000\n",
      "choc             0.000000\n",
      "manv             0.000000\n",
      "motor            0.000000\n",
      "occutc           0.987176\n",
      "dtype: float64\n",
      "\n",
      "Colonnes supprimées (trop de NaN > 50.0%): ['v2', 'lartpc', 'occutc']\n",
      "Colonne 'adr' : Imputation avec la valeur la plus fréquente (mode)\n",
      "Colonne 'voie' : Imputation avec 'Manquant' (catégorielle)\n",
      "Colonne 'an_nais' : Imputation avec la moyenne (numérique)\n",
      "\n",
      "Traitement des NaN terminé.\n",
      "\n",
      "Sélection des colonnes numériques importantes...\n",
      "Colonnes numériques retenues (corrélation > 0.1): ['place', 'catu', 'secu2', 'locp']\n",
      "Colonnes numériques retenues (variance > 0.1): ['jour', 'mois', 'lum', 'agg', 'int', 'atm', 'col', 'catr', 'v1', 'circ', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ', 'vma', 'place', 'catu', 'sexe', 'an_nais', 'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'motor']\n",
      "\n",
      "Sélection des colonnes catégorielles importantes...\n",
      "Colonnes catégorielles retenues (variance > 2): []\n",
      "\n",
      "Colonnes finales sélectionnées : ['secu1', 'secu3', 'sexe', 'prof', 'secu2', 'catr', 'agg', 'jour', 'senc', 'surf', 'obsm', 'atm', 'place', 'vma', 'an_nais', 'v1', 'manv', 'infra', 'locp', 'catv', 'trajet', 'lum', 'col', 'plan', 'int', 'choc', 'vosp', 'obs', 'motor', 'situ', 'catu', 'mois', 'circ']\n",
      "\n",
      "Résumé des colonnes importantes pour la prédiction :\n",
      "Nombre de colonnes finales : 33\n",
      "   secu1  secu3  sexe  prof  secu2  catr  agg  jour  senc  surf  ...  plan  \\\n",
      "0      2      1     1     1      0     4    2     7     1     2  ...     1   \n",
      "1      2      1     1     1      0     4    2     7     1     2  ...     1   \n",
      "2      1      1     2     1      0     3    2     7     2     2  ...     1   \n",
      "3      0      1     1     1      1     3    2     7     2     2  ...     1   \n",
      "4      2      0     1     1      6     3    2     7     1     2  ...     1   \n",
      "\n",
      "   int  choc  vosp  obs  motor  situ  catu  mois  circ  \n",
      "0    4     5     0    0      1     1     1     5     1  \n",
      "1    4     5     0    0      1     1     1     5     1  \n",
      "2    1     1     2    0      1     1     1     5     2  \n",
      "3    1     1     2    0      1     1     3     5     2  \n",
      "4    3     1     0    0      1     1     1     5     2  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Début de la recherche manuelle des hyperparamètres...\n",
      "Division des données en ensembles d'entraînement et de validation...\n",
      "Train size: 247472, Validation size: 61869\n",
      "Proportion de classes dans y_train : 2.446773776427232\n",
      "Proportion de classes dans y_val : 2.455009778726018\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=50, hidden_size=10 => F1=0.25414937402596394\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=50, hidden_size=20 => F1=0.2541034844738034\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=50, hidden_size=30 => F1=0.25419816931271855\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n",
      "Époque 50/100\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=100, hidden_size=10 => F1=0.25418741780405024\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n",
      "Époque 50/100\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=100, hidden_size=20 => F1=0.25418833279388703\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n",
      "Époque 50/100\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=100, hidden_size=30 => F1=0.25418741780405024\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/150\n",
      "Époque 50/150\n",
      "Époque 100/150\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=150, hidden_size=10 => F1=0.25428156392242096\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/150\n",
      "Époque 50/150\n",
      "Époque 100/150\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=150, hidden_size=20 => F1=0.25428156392242096\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/150\n",
      "Époque 50/150\n",
      "Époque 100/150\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.01, epochs=150, hidden_size=30 => F1=0.25428810034639626\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 1 1 1 1 1 3 4 1 1]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.05, epochs=50, hidden_size=10 => F1=0.28224339595322967\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 4 1 4 1 1 4 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.05, epochs=50, hidden_size=20 => F1=0.26314686104012963\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/50\n",
      "Prédictions : [1 4 1 1 1 1 1 1 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.05, epochs=50, hidden_size=30 => F1=0.24319714255388866\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n",
      "Époque 50/100\n",
      "Prédictions : [1 1 1 1 1 1 3 4 1 1]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.05, epochs=100, hidden_size=10 => F1=0.28132050351243826\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n",
      "Époque 50/100\n",
      "Prédictions : [1 4 1 1 1 1 3 4 1 4]\n",
      "Vraies valeurs : [1 1 1 3 1 1 1 3 1 4]\n",
      "Testé: lr=0.05, epochs=100, hidden_size=20 => F1=0.2949720778786479\n",
      "Initialisation du NeuralNetwork...\n",
      "Début de l'entraînement du NeuralNetwork...\n",
      "Époque 0/100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, epochs=500):\n",
    "        print(\"Initialisation du NeuralNetwork...\")\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size) * 0.1\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size) * 0.1\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def fit(self, X, y, batch_size=32):\n",
    "        print(\"Début de l'entraînement du NeuralNetwork...\")\n",
    "        y_one_hot = np.eye(self.output_size)[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Époque {epoch}/{self.epochs}\")\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y_one_hot[i:i + batch_size]\n",
    "\n",
    "                hidden_layer_activation = np.dot(X_batch, self.weights_input_hidden) + self.bias_hidden\n",
    "                hidden_layer_output = self.softmax(hidden_layer_activation)\n",
    "                final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "                output = self.softmax(final_layer_activation)\n",
    "\n",
    "                error = y_batch - output\n",
    "                d_output = error\n",
    "\n",
    "                d_hidden_layer = np.dot(d_output, self.weights_hidden_output.T) * self.softmax_derivative(hidden_layer_output)\n",
    "\n",
    "                self.weights_hidden_output += np.dot(hidden_layer_output.T, d_output) * self.learning_rate\n",
    "                self.bias_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "                self.weights_input_hidden += np.dot(X_batch.T, d_hidden_layer) * self.learning_rate\n",
    "                self.bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_layer_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_layer_output = self.softmax(hidden_layer_activation)\n",
    "        final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        output = self.softmax(final_layer_activation)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=500):\n",
    "        print(\"Initialisation de la LogisticRegression...\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Début de l'entraînement de la LogisticRegression...\")\n",
    "        self.weights = np.zeros((X.shape[1], len(np.unique(y))))\n",
    "        self.bias = np.zeros((1, len(np.unique(y))))\n",
    "        m = len(y)\n",
    "        y_one_hot = np.eye(len(np.unique(y)))[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Époque {epoch}/{self.epochs}\")\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self.softmax(linear_model)\n",
    "\n",
    "            dw = (1 / m) * np.dot(X.T, (predictions - y_one_hot))\n",
    "            db = (1 / m) * np.sum(predictions - y_one_hot, axis=0, keepdims=True)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self.softmax(linear_model)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "class XGBoost:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=3):\n",
    "        print(\"Initialisation de XGBoost...\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Début de l'entraînement de XGBoost...\")\n",
    "        y_one_hot = np.eye(len(np.unique(y)))[y]\n",
    "        residual = y_one_hot.copy()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            model = LogisticRegression(learning_rate=self.learning_rate, epochs=100)\n",
    "            model.fit(X, np.argmax(residual, axis=1))\n",
    "            predictions = model.predict(X)\n",
    "            predictions_one_hot = np.eye(len(np.unique(y)))[predictions]\n",
    "            residual -= predictions_one_hot\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], len(self.models[0].weights[0])))\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            predictions += np.eye(len(predictions[0]))[pred]\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    print(\"Division des données en ensembles d'entraînement et de validation...\")\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "def manual_hyperparameter_search(X, y):\n",
    "    print(\"Début de la recherche manuelle des hyperparamètres...\")\n",
    "    if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        X = np.array(X)\n",
    "    if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "        y = np.array(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
    "    print(f\"Proportion de classes dans y_train : {np.mean(y_train)}\")\n",
    "    print(f\"Proportion de classes dans y_val : {np.mean(y_val)}\")\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_conf_matrix = None\n",
    "\n",
    "    for learning_rate in [0.01, 0.05, 0.1]:\n",
    "        for epochs in [50, 100, 150]:  # Réduction des epochs pour éviter le sur-apprentissage\n",
    "            for hidden_size in [10, 20, 30]:\n",
    "                model = NeuralNetwork(input_size=X_train.shape[1], hidden_size=hidden_size, output_size=len(np.unique(y)), learning_rate=learning_rate, epochs=epochs)\n",
    "                model.fit(X_train, y_train)\n",
    "                f1, conf_matrix = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "                print(f\"Testé: lr={learning_rate}, epochs={epochs}, hidden_size={hidden_size} => F1={f1}\")\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = model\n",
    "                    best_conf_matrix = conf_matrix\n",
    "\n",
    "    for learning_rate in [0.1, 0.2]:\n",
    "        for n_estimators in [50, 100]:\n",
    "            model = XGBoost(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=3)\n",
    "            model.fit(X_train, y_train)\n",
    "            f1, conf_matrix = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "            print(f\"Testé: lr={learning_rate}, n_estimators={n_estimators} => F1={f1}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_conf_matrix = conf_matrix\n",
    "\n",
    "    print(\"Recherche manuelle terminée.\")\n",
    "    print(\"Meilleur F1:\", best_f1)\n",
    "    print(\"Matrice de confusion:\\n\", best_conf_matrix)\n",
    "    return best_model, best_f1, best_conf_matrix\n",
    "\n",
    "\n",
    "def f1_score_manual(y_true, y_pred):\n",
    "    f1_scores = []\n",
    "    for label in np.unique(y_true):\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_true == label) & (y_pred != label))\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            f1_scores.append(0)\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def confusion_matrix_manual(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    print(\"Prédictions :\", predictions[:10])\n",
    "    print(\"Vraies valeurs :\", y_val[:10])\n",
    "    f1 = f1_score_manual(y_val, predictions)\n",
    "    conf_matrix = confusion_matrix_manual(y_val, predictions)\n",
    "    return f1, conf_matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Début du script...\")\n",
    "    X, y = preprocess_data(\"data/caract-2023.csv\", \"data/lieux-2023.csv\", \"data/usagers-2023.csv\", \"data/vehicules-2023.csv\", \"Num_Acc\", \"grav\")\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    best_model, best_f1, best_conf_matrix = manual_hyperparameter_search(X, y)\n",
    "\n",
    "    with open(\"best_model.pkl\", \"wb\") as f:\n",
    "        np.save(f, best_model)\n",
    "\n",
    "    print(\"Modèle sauvegardé dans 'best_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a165037-d5cd-4232-94f4-5a2f7685023f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
