{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504405fd-6139-49ca-b7ad-97e7073758f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b384861e-0bc8-4b58-833b-9298abb4ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(data1, data2, data3, data4, common_key, target_column):\n",
    "    \"\"\"Charge, fusionne et nettoie les fichiers CSV.\"\"\"\n",
    "    caract = pd.read_csv(data1, sep=\";\", low_memory=False)\n",
    "    lieux = pd.read_csv(data2, sep=\";\", low_memory=False)\n",
    "    usagers = pd.read_csv(data3, sep=\";\", low_memory=False)\n",
    "    vehicules = pd.read_csv(data4, sep=\";\", low_memory=False)\n",
    "    \n",
    "    merged_df = caract.merge(lieux, on=common_key, how=\"inner\") \\\n",
    "                      .merge(usagers, on=common_key, how=\"inner\") \\\n",
    "                      .merge(vehicules, on=common_key, how=\"inner\")\n",
    "    \n",
    "    print(f\"Colonnes disponibles après fusion : {merged_df.columns.tolist()}\")\n",
    "\n",
    "    merged_df = merged_df.dropna(axis=1, how='all')\n",
    "    merged_df = merged_df.loc[:, merged_df.nunique() > 1]\n",
    "    \n",
    "    y = merged_df[target_column]\n",
    "    X = merged_df.drop(columns=[target_column, common_key])\n",
    "    return X, y\n",
    "\n",
    "def correlation_matrix(X, y, threshold=0.1):\n",
    "    \"\"\"Filtre les colonnes numériques basées sur leur corrélation avec la cible.\"\"\"\n",
    "    correlations = {}\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        corr = np.corrcoef(X[col], y)[0, 1]\n",
    "        correlations[col] = corr\n",
    "    selected = [col for col, corr in correlations.items() if abs(corr) > threshold]\n",
    "    print(f\"Colonnes numériques retenues (corrélation > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def categorical_analysis(X, y, threshold=0.2):\n",
    "    \"\"\"Filtre les colonnes catégorielles en se basant sur la variance des moyennes.\"\"\"\n",
    "    X_with_target = X.copy()\n",
    "    X_with_target['target'] = y\n",
    "\n",
    "    selected = []\n",
    "    for col in X.select_dtypes(include='object').columns:\n",
    "        means = X_with_target.groupby(col)['target'].mean()\n",
    "        if means.var() > threshold:\n",
    "            selected.append(col)\n",
    "    print(f\"Colonnes catégorielles retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def low_variance_filter(X, threshold=0.01):\n",
    "    \"\"\"Filtre les colonnes numériques avec une faible variance.\"\"\"\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    variances = X_numeric.var()\n",
    "    selected = variances[variances > threshold].index.tolist()\n",
    "    print(f\"Colonnes numériques retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "\n",
    "def auto_handle_nan(df, nan_threshold_delete=0.5, nan_threshold_impute=0.1):\n",
    "    \"\"\"\n",
    "    Traite automatiquement les valeurs NaN dans un dataset.\n",
    "    - Supprime les colonnes avec trop de NaN.\n",
    "    - Impute (remplace) les NaN avec des stratégies adaptées :\n",
    "      - Moyenne pour colonnes numériques.\n",
    "      - Mode ou \"Inconnu\" pour colonnes catégorielles.\n",
    "    \"\"\"\n",
    "    print(\"Analyse des NaN dans le dataset...\\n\")\n",
    "    \n",
    "    nan_percent = df.isnull().mean()\n",
    "    print(\"Pourcentage de valeurs manquantes par colonne :\")\n",
    "    print(nan_percent)\n",
    "    \n",
    "    cols_to_delete = nan_percent[nan_percent > nan_threshold_delete].index\n",
    "    print(f\"\\nColonnes supprimées (trop de NaN > {nan_threshold_delete*100}%): {list(cols_to_delete)}\")\n",
    "    df = df.drop(columns=cols_to_delete)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if df[col].dtype == 'object':\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec 'Manquant' (catégorielle)\")\n",
    "                    df[col] = df[col].fillna(\"Manquant\")\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la valeur la plus fréquente (mode)\")\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la médiane (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la moyenne (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    print(\"\\nTraitement des NaN terminé.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4696f6e-5924-4dcb-9cd1-75e13bc64fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data1, data2, data3, data4, common_key, target_column):\n",
    "    print(\"Chargement et fusion des données...\")\n",
    "    X, y = load_and_process_data(data1, data2, data3, data4, common_key, target_column)\n",
    "\n",
    "    print(\"\\nSuppression des doublons...\")\n",
    "    print(\"Nombre de doublons avant suppression :\", X.duplicated().sum())\n",
    "    X = X.drop_duplicates()\n",
    "    print(\"Nombre de doublons après suppression :\", X.duplicated().sum())\n",
    "\n",
    "    print(\"\\nTraitement des valeurs NaN...\")\n",
    "    X = auto_handle_nan(X)\n",
    "\n",
    "    print(\"\\nSélection des colonnes numériques importantes...\")\n",
    "    numeric_cols_corr = correlation_matrix(X, y, threshold=0.1)\n",
    "    numeric_cols_var = low_variance_filter(X, threshold=0.1)\n",
    "\n",
    "    print(\"\\nSélection des colonnes catégorielles importantes...\")\n",
    "    categorical_cols = categorical_analysis(X, y, threshold=2)\n",
    "\n",
    "    selected_columns = list(set(numeric_cols_var + categorical_cols))\n",
    "    print(f\"\\nColonnes finales sélectionnées : {selected_columns}\")\n",
    "\n",
    "    X_filtered = X[selected_columns]\n",
    "    # final_data = pd.concat([X_filtered, y], axis=1)\n",
    "\n",
    "    print(\"\\nRésumé des colonnes importantes pour la prédiction :\")\n",
    "    print(f\"Nombre de colonnes finales : {len(X_filtered.columns)}\")\n",
    "    print(X_filtered.head())\n",
    "    return X_filtered, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e17c8-41d2-46ae-9fd9-e4595dd5be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement et fusion des données...\n",
      "Colonnes disponibles après fusion : ['Num_Acc', 'jour', 'mois', 'an', 'hrmn', 'lum', 'dep', 'com', 'agg', 'int', 'atm', 'col', 'adr', 'lat', 'long', 'catr', 'voie', 'v1', 'v2', 'circ', 'nbv', 'vosp', 'prof', 'pr', 'pr1', 'plan', 'lartpc', 'larrout', 'surf', 'infra', 'situ', 'vma', 'id_usager', 'id_vehicule_x', 'num_veh_x', 'place', 'catu', 'grav', 'sexe', 'an_nais', 'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'actp', 'etatp', 'id_vehicule_y', 'num_veh_y', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'motor', 'occutc']\n",
      "\n",
      "Suppression des doublons...\n",
      "Nombre de doublons avant suppression : 0\n",
      "Nombre de doublons après suppression : 0\n",
      "\n",
      "Traitement des valeurs NaN...\n",
      "Analyse des NaN dans le dataset...\n",
      "\n",
      "Pourcentage de valeurs manquantes par colonne :\n",
      "jour             0.000000\n",
      "mois             0.000000\n",
      "hrmn             0.000000\n",
      "lum              0.000000\n",
      "dep              0.000000\n",
      "com              0.000000\n",
      "agg              0.000000\n",
      "int              0.000000\n",
      "atm              0.000000\n",
      "col              0.000000\n",
      "adr              0.025318\n",
      "lat              0.000000\n",
      "long             0.000000\n",
      "catr             0.000000\n",
      "voie             0.163079\n",
      "v1               0.000000\n",
      "v2               0.917053\n",
      "circ             0.000000\n",
      "nbv              0.000000\n",
      "vosp             0.000000\n",
      "prof             0.000000\n",
      "pr               0.000000\n",
      "pr1              0.000000\n",
      "plan             0.000000\n",
      "lartpc           0.999483\n",
      "larrout          0.000000\n",
      "surf             0.000000\n",
      "infra            0.000000\n",
      "situ             0.000000\n",
      "vma              0.000000\n",
      "id_usager        0.000000\n",
      "id_vehicule_x    0.000000\n",
      "num_veh_x        0.000000\n",
      "place            0.000000\n",
      "catu             0.000000\n",
      "sexe             0.000000\n",
      "an_nais          0.020547\n",
      "trajet           0.000000\n",
      "secu1            0.000000\n",
      "secu2            0.000000\n",
      "secu3            0.000000\n",
      "locp             0.000000\n",
      "actp             0.000000\n",
      "etatp            0.000000\n",
      "id_vehicule_y    0.000000\n",
      "num_veh_y        0.000000\n",
      "senc             0.000000\n",
      "catv             0.000000\n",
      "obs              0.000000\n",
      "obsm             0.000000\n",
      "choc             0.000000\n",
      "manv             0.000000\n",
      "motor            0.000000\n",
      "occutc           0.987176\n",
      "dtype: float64\n",
      "\n",
      "Colonnes supprimées (trop de NaN > 50.0%): ['v2', 'lartpc', 'occutc']\n",
      "Colonne 'adr' : Imputation avec la valeur la plus fréquente (mode)\n",
      "Colonne 'voie' : Imputation avec 'Manquant' (catégorielle)\n",
      "Colonne 'an_nais' : Imputation avec la moyenne (numérique)\n",
      "\n",
      "Traitement des NaN terminé.\n",
      "\n",
      "Sélection des colonnes numériques importantes...\n",
      "Colonnes numériques retenues (corrélation > 0.1): ['place', 'catu', 'sexe', 'secu1', 'secu2', 'locp', 'etatp']\n",
      "Colonnes numériques retenues (variance > 0.1): ['jour', 'mois', 'lum', 'agg', 'int', 'atm', 'col', 'catr', 'v1', 'circ', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ', 'vma', 'place', 'catu', 'sexe', 'an_nais', 'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'etatp', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'motor']\n",
      "\n",
      "Sélection des colonnes catégorielles importantes...\n",
      "Colonnes catégorielles retenues (variance > 2): []\n",
      "\n",
      "Colonnes finales sélectionnées : ['catv', 'obs', 'col', 'mois', 'int', 'sexe', 'agg', 'surf', 'secu3', 'vosp', 'jour', 'senc', 'plan', 'an_nais', 'etatp', 'catu', 'place', 'situ', 'trajet', 'manv', 'choc', 'secu2', 'catr', 'v1', 'locp', 'infra', 'lum', 'circ', 'atm', 'vma', 'prof', 'obsm', 'motor', 'secu1']\n",
      "\n",
      "Résumé des colonnes importantes pour la prédiction :\n",
      "Nombre de colonnes finales : 34\n",
      "   catv  obs  col  mois  int  sexe  agg  surf  secu3  vosp  ...  locp  infra  \\\n",
      "0    30    0    7     5    4     1    2     2     -1     0  ...    -1      0   \n",
      "1    30    0    7     5    4     1    2     2     -1     0  ...    -1      0   \n",
      "2     7    0    6     5    1     2    2     2     -1     2  ...    -1      0   \n",
      "3     7    0    6     5    1     1    2     2     -1     2  ...     2      0   \n",
      "4     2    0    1     5    3     1    2     2      0     0  ...     0      5   \n",
      "\n",
      "   lum  circ  atm  vma  prof  obsm  motor  secu1  \n",
      "0    1     1    2   30     1     0      1      2  \n",
      "1    1     1    2   30     1     0      1      2  \n",
      "2    5     2    3   50     1     1      1      1  \n",
      "3    5     2    3   50     1     1      1      0  \n",
      "4    1     2    2   50     1     2      1      2  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-18 11:37:15,275] A new study created in memory with name: no-name-62c0527c-ee10-49f0-8873-1ad4ae631c97\n",
      "/var/folders/6_/z1bmyj3n2tn0lxd9mp0y82m40000gn/T/ipykernel_26835/1219335438.py:95: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, epochs=500):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size) * 0.1\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size) * 0.1\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            hidden_layer_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "            hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "            final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "            output = self.sigmoid(final_layer_activation)\n",
    "            \n",
    "            error = y - output\n",
    "            \n",
    "            d_output = error * self.sigmoid_derivative(output)\n",
    "            d_hidden_layer = np.dot(d_output, self.weights_hidden_output.T) * self.sigmoid_derivative(hidden_layer_output)\n",
    "            \n",
    "            self.weights_hidden_output += np.dot(hidden_layer_output.T, d_output) * self.learning_rate\n",
    "            self.bias_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "            self.weights_input_hidden += np.dot(X.T, d_hidden_layer) * self.learning_rate\n",
    "            self.bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
    "    \n",
    "    def predict(self, X):\n",
    "        hidden_layer_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "        final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        output = self.sigmoid(final_layer_activation)\n",
    "        return np.round(output)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        m = len(y)\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = 1 / (1 + np.exp(-linear_model))\n",
    "            \n",
    "            dw = (1/m) * np.dot(X.T, (predictions - y))\n",
    "            db = (1/m) * np.sum(predictions - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = 1 / (1 + np.exp(-linear_model))\n",
    "        return np.round(predictions)\n",
    "\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    \n",
    "    # Conversion en NumPy\n",
    "    X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.to_numpy() if isinstance(y, pd.Series) else y\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def optimize_model(trial, model_name, X_train, y_train, X_val, y_val):\n",
    "    if model_name == \"NeuralNetwork\":\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 5, 50)\n",
    "        epochs = trial.suggest_int(\"epochs\", 100, 1000)\n",
    "        \n",
    "        model = NeuralNetwork(input_size=X_train.shape[1], hidden_size=hidden_size, output_size=1, learning_rate=learning_rate, epochs=epochs)\n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1)\n",
    "        epochs = trial.suggest_int(\"epochs\", 100, 1000)\n",
    "        \n",
    "        model = LogisticRegression(learning_rate=learning_rate, epochs=epochs)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    return accuracy_score(y_val, predictions)\n",
    "\n",
    "def auto_ml(X, y, models):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    final_model = None\n",
    "    \n",
    "    for model_name in models:\n",
    "        def objective(trial):\n",
    "            return optimize_model(trial, model_name, X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=30)  # 30 essais par modèle\n",
    "        \n",
    "        if study.best_value > best_score:\n",
    "            best_model_name = model_name\n",
    "            best_params = study.best_params\n",
    "            best_score = study.best_value\n",
    "            \n",
    "            final_model = optimize_model(trial, model_name, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    return final_model, best_model_name, best_params, best_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X,y = preprocess_data(\"data/caract-2023.csv\", \"data/lieux-2023.csv\", \"data/usagers-2023.csv\", \"data/vehicules-2023.csv\", \"Num_Acc\", \"grav\")\n",
    "    models = [\"NeuralNetwork\", \"LogisticRegression\"]\n",
    "    \n",
    "    final_model, best_model_name, best_params, best_score = auto_ml(X, y, models)\n",
    "    \n",
    "    print(\"Meilleur modèle :\", best_model_name)\n",
    "    print(\"Meilleurs hyperparamètres :\", best_params)\n",
    "    print(\"Meilleure précision :\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a165037-d5cd-4232-94f4-5a2f7685023f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
